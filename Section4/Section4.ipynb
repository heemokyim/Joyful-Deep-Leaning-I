{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set 4\n",
    "\n",
    "## Solutions of Rex Wang\n",
    "Designed by Ben Usman, Kun He, and Sarah Adel Bargal, with help from Kate Saenko and Brian Kulis.\n",
    "\n",
    "This assignment will introduce you to:\n",
    "1. Building and training a convolutional network\n",
    "2. Saving snapshots of your trained model\n",
    "3. Reloading weights from a saved model\n",
    "4. Fine-tuning a pre-trained network\n",
    "5. Visualizations using Tensorboard\n",
    "\n",
    "This code has been tested and should for Python 3.5 and 2.7 with tensorflow 1.0.*. Since recently, you can update to recent tensorflow version just by doing `pip install tensorflow`,  or `pip install tensorflow-gpu` if you want to use GPU.\n",
    "\n",
    "**Note:** This notebook contains problem descriptions and demo/starter code. However, you're welcome to implement and submit .py files directly, if that's easier for you. Starter .py files are provided in the same `pset4/` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Tutorials\n",
    "\n",
    "You will find these TensorFlow tutorials on CNNs useful:\n",
    " - [Deep MNIST for experts](https://www.tensorflow.org/get_started/mnist/pros)\n",
    " - [Convolutional Neural Networks](https://www.tensorflow.org/tutorials/deep_cnn)\n",
    " \n",
    "Note that there are many ways to implement the same thing in TensorFlow, for example, both tf.nn and tf.layers provide convolutional layers but with slightly different interfaces. You will need to read the documentation of the functions provided below to understand how they work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Building and Training a ConvNet on SVHN\n",
    "(25 points)\n",
    "\n",
    "First we provide demo code that trains a convolutional network on the [SVHN Dataset](http://ufldl.stanford.edu/housenumbers/).. \n",
    "\n",
    "You will need to download   __Format 2__ from the link above.\n",
    "- Create a directory named `svhn_mat/` in the working directory. Or, you can create it anywhere you want, but change the path in `svhn_dataset_generator` to match it.\n",
    "- Download `train_32x32.mat` and `test_32x32.mat` to this directory.\n",
    "- `extra_32x32.mat` is NOT needed.\n",
    "- You may find the `wget` command useful for downloading on linux. \n",
    "\n",
    "\n",
    "\n",
    "The following defines a generator for the SVHN Dataset, yielding the next batch every time next is invoked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.io\n",
    "\n",
    "from six.moves import range\n",
    "\n",
    "import read_data\n",
    "\n",
    "@read_data.restartable\n",
    "def svhn_dataset_generator(dataset_name, batch_size):\n",
    "    assert dataset_name in ['train', 'test']\n",
    "    assert batch_size > 0 or batch_size == -1  # -1 for entire dataset\n",
    "    \n",
    "    path = './svhn_mat/' # path to the SVHN dataset you will download in Q1.1\n",
    "    file_name = '%s_32x32.mat' % dataset_name\n",
    "    file_dict = scipy.io.loadmat(os.path.join(path, file_name))\n",
    "    X_all = file_dict['X'].transpose((3, 0, 1, 2))\n",
    "    y_all = file_dict['y']\n",
    "    data_len = X_all.shape[0]\n",
    "    batch_size = batch_size if batch_size > 0 else data_len\n",
    "    \n",
    "    X_all_padded = np.concatenate([X_all, X_all[:batch_size]], axis=0)\n",
    "    y_all_padded = np.concatenate([y_all, y_all[:batch_size]], axis=0)\n",
    "    y_all_padded[y_all_padded == 10] = 0\n",
    "    \n",
    "    for slice_i in range(int(math.ceil(data_len / batch_size))):\n",
    "        idx = slice_i * batch_size\n",
    "        X_batch = X_all_padded[idx:idx + batch_size]\n",
    "        y_batch = np.ravel(y_all_padded[idx:idx + batch_size])\n",
    "        yield X_batch, y_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following defines the CovNet Model. It has two identical conv layers with 32 5x5 convlution filters, followed by a fully-connected layer to output the logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def cnn_map(x_):\n",
    "    with tf.variable_scope('conv1'):\n",
    "        conv1 = tf.layers.conv2d(\n",
    "                inputs=x_,\n",
    "                filters=32,  # number of filters\n",
    "                kernel_size=[5, 5],\n",
    "                padding=\"same\",\n",
    "                activation=tf.nn.relu)\n",
    "    \n",
    "    pool1 = tf.layers.max_pooling2d(inputs=conv1, \n",
    "                                    pool_size=[2, 2], \n",
    "                                    strides=2)  # convolution stride\n",
    "    \n",
    "    conv2 = tf.layers.conv2d(\n",
    "            inputs=pool1,\n",
    "            filters=32, # number of filters\n",
    "            kernel_size=[5, 5],\n",
    "            padding=\"same\",\n",
    "            activation=tf.nn.relu)\n",
    "    \n",
    "    pool2 = tf.layers.max_pooling2d(inputs=conv2, \n",
    "                                    pool_size=[2, 2], \n",
    "                                    strides=2)  # convolution stride\n",
    "        \n",
    "    pool_flat = tf.contrib.layers.flatten(pool2, scope='pool2flat')\n",
    "    dense = tf.layers.dense(inputs=pool_flat, units=500, activation=tf.nn.relu)\n",
    "    logits = tf.layers.dense(inputs=dense, units=10)\n",
    "    return logits\n",
    "\n",
    "\n",
    "def apply_classification_loss(model_function):\n",
    "    with tf.Graph().as_default() as g:\n",
    "        with tf.device(\"/gpu:0\"):  # use gpu:0 if on GPU\n",
    "            x_ = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "            y_ = tf.placeholder(tf.int32, [None])\n",
    "            y_logits = model_function(x_)\n",
    "            \n",
    "            y_dict = dict(labels=y_, logits=y_logits)\n",
    "            losses = tf.nn.sparse_softmax_cross_entropy_with_logits(**y_dict)\n",
    "            cross_entropy_loss = tf.reduce_mean(losses)\n",
    "            trainer = tf.train.AdamOptimizer()\n",
    "            train_op = trainer.minimize(cross_entropy_loss)\n",
    "            \n",
    "            y_pred = tf.argmax(tf.nn.softmax(y_logits), dimension=1)\n",
    "            correct_prediction = tf.equal(tf.cast(y_pred, tf.int32), y_)\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    model_dict = {'graph': g, 'inputs': [x_, y_], 'train_op': train_op,\n",
    "                  'accuracy': accuracy, 'loss': cross_entropy_loss}\n",
    "    \n",
    "    return model_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1.2 Training SVHN Net\n",
    "Now we train a `cnn_map` net on Format 2 of the SVHN Dataset. We will call this \"SVHN net\". \n",
    "\n",
    "**Note:** training will take a while, so you might want to use GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model_dict, dataset_generators, epoch_n, print_every):\n",
    "    with model_dict['graph'].as_default(), tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        for epoch_i in range(epoch_n):\n",
    "            for iter_i, data_batch in enumerate(dataset_generators['train']):\n",
    "                train_feed_dict = dict(zip(model_dict['inputs'], data_batch))\n",
    "                sess.run(model_dict['train_op'], feed_dict=train_feed_dict)\n",
    "                \n",
    "                if iter_i % print_every == 0:\n",
    "                    collect_arr = []\n",
    "                    for test_batch in dataset_generators['test']:\n",
    "                        test_feed_dict = dict(zip(model_dict['inputs'], test_batch))\n",
    "                        to_compute = [model_dict['loss'], model_dict['accuracy']]\n",
    "                        collect_arr.append(sess.run(to_compute, test_feed_dict))\n",
    "                    averages = np.mean(collect_arr, axis=0)\n",
    "                    fmt = (epoch_i, iter_i, ) + tuple(averages)\n",
    "                    print('epoch {:d} iter {:d}, loss: {:.3f}, '\n",
    "                          'accuracy: {:.3f}'.format(*fmt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 iter 0, loss: 145.154, accuracy: 0.196\n",
      "epoch 0 iter 200, loss: 1.098, accuracy: 0.664\n",
      "epoch 1 iter 0, loss: 1.009, accuracy: 0.696\n",
      "epoch 1 iter 200, loss: 0.914, accuracy: 0.733\n",
      "epoch 2 iter 0, loss: 0.908, accuracy: 0.735\n",
      "epoch 2 iter 200, loss: 0.861, accuracy: 0.753\n",
      "epoch 3 iter 0, loss: 0.903, accuracy: 0.737\n",
      "epoch 3 iter 200, loss: 0.898, accuracy: 0.748\n",
      "epoch 4 iter 0, loss: 0.885, accuracy: 0.748\n",
      "epoch 4 iter 200, loss: 0.856, accuracy: 0.766\n",
      "epoch 5 iter 0, loss: 0.814, accuracy: 0.773\n",
      "epoch 5 iter 200, loss: 0.842, accuracy: 0.770\n",
      "epoch 6 iter 0, loss: 0.811, accuracy: 0.778\n",
      "epoch 6 iter 200, loss: 0.862, accuracy: 0.773\n",
      "epoch 7 iter 0, loss: 0.813, accuracy: 0.782\n",
      "epoch 7 iter 200, loss: 0.901, accuracy: 0.775\n",
      "epoch 8 iter 0, loss: 0.820, accuracy: 0.783\n",
      "epoch 8 iter 200, loss: 0.942, accuracy: 0.771\n",
      "epoch 9 iter 0, loss: 0.816, accuracy: 0.785\n",
      "epoch 9 iter 200, loss: 1.012, accuracy: 0.767\n",
      "epoch 10 iter 0, loss: 0.858, accuracy: 0.779\n",
      "epoch 10 iter 200, loss: 0.991, accuracy: 0.775\n",
      "epoch 11 iter 0, loss: 0.904, accuracy: 0.778\n",
      "epoch 11 iter 200, loss: 0.992, accuracy: 0.779\n",
      "epoch 12 iter 0, loss: 1.029, accuracy: 0.767\n",
      "epoch 12 iter 200, loss: 1.018, accuracy: 0.776\n",
      "epoch 13 iter 0, loss: 1.039, accuracy: 0.767\n",
      "epoch 13 iter 200, loss: 1.070, accuracy: 0.776\n",
      "epoch 14 iter 0, loss: 1.091, accuracy: 0.763\n",
      "epoch 14 iter 200, loss: 1.109, accuracy: 0.776\n",
      "epoch 15 iter 0, loss: 1.170, accuracy: 0.757\n",
      "epoch 15 iter 200, loss: 1.144, accuracy: 0.774\n",
      "epoch 16 iter 0, loss: 1.131, accuracy: 0.772\n",
      "epoch 16 iter 200, loss: 1.172, accuracy: 0.778\n",
      "epoch 17 iter 0, loss: 1.171, accuracy: 0.774\n",
      "epoch 17 iter 200, loss: 1.230, accuracy: 0.775\n",
      "epoch 18 iter 0, loss: 1.164, accuracy: 0.773\n",
      "epoch 18 iter 200, loss: 1.272, accuracy: 0.780\n",
      "epoch 19 iter 0, loss: 1.221, accuracy: 0.769\n",
      "epoch 19 iter 200, loss: 1.288, accuracy: 0.784\n",
      "epoch 20 iter 0, loss: 1.207, accuracy: 0.771\n",
      "epoch 20 iter 200, loss: 1.331, accuracy: 0.781\n",
      "epoch 21 iter 0, loss: 1.340, accuracy: 0.758\n",
      "epoch 21 iter 200, loss: 1.343, accuracy: 0.781\n",
      "epoch 22 iter 0, loss: 1.424, accuracy: 0.762\n",
      "epoch 22 iter 200, loss: 1.321, accuracy: 0.784\n",
      "epoch 23 iter 0, loss: 1.348, accuracy: 0.767\n",
      "epoch 23 iter 200, loss: 1.334, accuracy: 0.791\n",
      "epoch 24 iter 0, loss: 1.427, accuracy: 0.765\n",
      "epoch 24 iter 200, loss: 1.221, accuracy: 0.792\n",
      "epoch 25 iter 0, loss: 1.301, accuracy: 0.782\n",
      "epoch 25 iter 200, loss: 1.418, accuracy: 0.788\n",
      "epoch 26 iter 0, loss: 1.475, accuracy: 0.775\n",
      "epoch 26 iter 200, loss: 1.417, accuracy: 0.800\n",
      "epoch 27 iter 0, loss: 1.559, accuracy: 0.778\n",
      "epoch 27 iter 200, loss: 1.476, accuracy: 0.797\n",
      "epoch 28 iter 0, loss: 1.659, accuracy: 0.782\n",
      "epoch 28 iter 200, loss: 1.479, accuracy: 0.796\n",
      "epoch 29 iter 0, loss: 1.648, accuracy: 0.754\n",
      "epoch 29 iter 200, loss: 1.641, accuracy: 0.791\n",
      "epoch 30 iter 0, loss: 1.647, accuracy: 0.761\n",
      "epoch 30 iter 200, loss: 1.723, accuracy: 0.793\n",
      "epoch 31 iter 0, loss: 1.841, accuracy: 0.774\n",
      "epoch 31 iter 200, loss: 2.009, accuracy: 0.787\n",
      "epoch 32 iter 0, loss: 1.873, accuracy: 0.779\n",
      "epoch 32 iter 200, loss: 2.125, accuracy: 0.784\n",
      "epoch 33 iter 0, loss: 2.056, accuracy: 0.790\n",
      "epoch 33 iter 200, loss: 2.519, accuracy: 0.783\n",
      "epoch 34 iter 0, loss: 2.215, accuracy: 0.778\n",
      "epoch 34 iter 200, loss: 2.497, accuracy: 0.789\n",
      "epoch 35 iter 0, loss: 2.309, accuracy: 0.781\n",
      "epoch 35 iter 200, loss: 2.619, accuracy: 0.787\n",
      "epoch 36 iter 0, loss: 2.456, accuracy: 0.768\n",
      "epoch 36 iter 200, loss: 2.919, accuracy: 0.783\n",
      "epoch 37 iter 0, loss: 2.564, accuracy: 0.784\n",
      "epoch 37 iter 200, loss: 2.741, accuracy: 0.780\n",
      "epoch 38 iter 0, loss: 2.672, accuracy: 0.786\n",
      "epoch 38 iter 200, loss: 2.639, accuracy: 0.784\n",
      "epoch 39 iter 0, loss: 2.824, accuracy: 0.791\n",
      "epoch 39 iter 200, loss: 2.561, accuracy: 0.787\n",
      "epoch 40 iter 0, loss: 2.643, accuracy: 0.798\n",
      "epoch 40 iter 200, loss: 2.565, accuracy: 0.782\n",
      "epoch 41 iter 0, loss: 3.102, accuracy: 0.789\n",
      "epoch 41 iter 200, loss: 2.796, accuracy: 0.799\n",
      "epoch 42 iter 0, loss: 3.046, accuracy: 0.795\n",
      "epoch 42 iter 200, loss: 2.667, accuracy: 0.791\n",
      "epoch 43 iter 0, loss: 2.948, accuracy: 0.805\n",
      "epoch 43 iter 200, loss: 2.717, accuracy: 0.794\n",
      "epoch 44 iter 0, loss: 3.160, accuracy: 0.799\n",
      "epoch 44 iter 200, loss: 2.564, accuracy: 0.779\n",
      "epoch 45 iter 0, loss: 3.151, accuracy: 0.806\n",
      "epoch 45 iter 200, loss: 2.716, accuracy: 0.777\n",
      "epoch 46 iter 0, loss: 3.168, accuracy: 0.798\n",
      "epoch 46 iter 200, loss: 2.762, accuracy: 0.790\n",
      "epoch 47 iter 0, loss: 3.294, accuracy: 0.807\n",
      "epoch 47 iter 200, loss: 2.841, accuracy: 0.789\n",
      "epoch 48 iter 0, loss: 3.366, accuracy: 0.811\n",
      "epoch 48 iter 200, loss: 2.889, accuracy: 0.787\n",
      "epoch 49 iter 0, loss: 3.569, accuracy: 0.808\n",
      "epoch 49 iter 200, loss: 2.809, accuracy: 0.792\n"
     ]
    }
   ],
   "source": [
    "dataset_generators = {\n",
    "        'train': svhn_dataset_generator('train', 256),\n",
    "        'test': svhn_dataset_generator('test', 256)\n",
    "}\n",
    "    \n",
    "model_dict = apply_classification_loss(cnn_map)\n",
    "train_model(model_dict, dataset_generators, epoch_n=50, print_every=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1.3 SVHN Net Variations\n",
    "Now we vary the structure of the network. To keep things simple, we still use  two identical conv layers, but vary their parameters. \n",
    "\n",
    "Report the final test accuracy on 3 different number of filters, and 3 different number of strides. Each time when you vary one parameter, keep the other fixed at the original value.\n",
    "\n",
    "|Stride|Accuracy|\n",
    "|--|-------------------------------|\n",
    "| 4 | 0.833 |\n",
    "| 8 | 0.750 |\n",
    "| 16 | 0.570 |\n",
    "\n",
    "|Filters|Accuracy|\n",
    "|--|-------------------------------|\n",
    "| 24 | 0.821 |\n",
    "| 16 | 0.818 |\n",
    "| 8 | 0.821 |\n",
    "\n",
    "A template for one sample modification is given below. \n",
    "\n",
    "**Note:** you're welcome to decide how many training epochs to use, if that gets you the same results but faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 iter 0, loss: 8.938, accuracy: 0.077\n",
      "epoch 0 iter 200, loss: 1.950, accuracy: 0.322\n",
      "epoch 1 iter 0, loss: 1.748, accuracy: 0.406\n",
      "epoch 1 iter 200, loss: 1.679, accuracy: 0.431\n",
      "epoch 2 iter 0, loss: 1.663, accuracy: 0.439\n",
      "epoch 2 iter 200, loss: 1.589, accuracy: 0.468\n",
      "epoch 3 iter 0, loss: 1.518, accuracy: 0.491\n",
      "epoch 3 iter 200, loss: 1.470, accuracy: 0.511\n",
      "epoch 4 iter 0, loss: 1.488, accuracy: 0.508\n",
      "epoch 4 iter 200, loss: 1.464, accuracy: 0.514\n",
      "epoch 5 iter 0, loss: 1.472, accuracy: 0.515\n",
      "epoch 5 iter 200, loss: 1.398, accuracy: 0.536\n",
      "epoch 6 iter 0, loss: 1.491, accuracy: 0.509\n",
      "epoch 6 iter 200, loss: 1.405, accuracy: 0.539\n",
      "epoch 7 iter 0, loss: 1.518, accuracy: 0.504\n",
      "epoch 7 iter 200, loss: 1.387, accuracy: 0.542\n",
      "epoch 8 iter 0, loss: 1.495, accuracy: 0.512\n",
      "epoch 8 iter 200, loss: 1.376, accuracy: 0.547\n",
      "epoch 9 iter 0, loss: 1.464, accuracy: 0.520\n",
      "epoch 9 iter 200, loss: 1.343, accuracy: 0.560\n",
      "epoch 10 iter 0, loss: 1.444, accuracy: 0.529\n",
      "epoch 10 iter 200, loss: 1.371, accuracy: 0.550\n",
      "epoch 11 iter 0, loss: 1.429, accuracy: 0.531\n",
      "epoch 11 iter 200, loss: 1.342, accuracy: 0.558\n",
      "epoch 12 iter 0, loss: 1.417, accuracy: 0.533\n",
      "epoch 12 iter 200, loss: 1.348, accuracy: 0.554\n",
      "epoch 13 iter 0, loss: 1.439, accuracy: 0.533\n",
      "epoch 13 iter 200, loss: 1.343, accuracy: 0.557\n",
      "epoch 14 iter 0, loss: 1.443, accuracy: 0.526\n",
      "epoch 14 iter 200, loss: 1.364, accuracy: 0.551\n",
      "epoch 15 iter 0, loss: 1.436, accuracy: 0.531\n",
      "epoch 15 iter 200, loss: 1.393, accuracy: 0.542\n",
      "epoch 16 iter 0, loss: 1.420, accuracy: 0.535\n",
      "epoch 16 iter 200, loss: 1.365, accuracy: 0.555\n",
      "epoch 17 iter 0, loss: 1.367, accuracy: 0.550\n",
      "epoch 17 iter 200, loss: 1.370, accuracy: 0.552\n",
      "epoch 18 iter 0, loss: 1.389, accuracy: 0.545\n",
      "epoch 18 iter 200, loss: 1.345, accuracy: 0.558\n",
      "epoch 19 iter 0, loss: 1.415, accuracy: 0.538\n",
      "epoch 19 iter 200, loss: 1.332, accuracy: 0.561\n",
      "epoch 20 iter 0, loss: 1.400, accuracy: 0.543\n",
      "epoch 20 iter 200, loss: 1.331, accuracy: 0.564\n",
      "epoch 21 iter 0, loss: 1.405, accuracy: 0.546\n",
      "epoch 21 iter 200, loss: 1.339, accuracy: 0.567\n",
      "epoch 22 iter 0, loss: 1.392, accuracy: 0.548\n",
      "epoch 22 iter 200, loss: 1.360, accuracy: 0.561\n",
      "epoch 23 iter 0, loss: 1.376, accuracy: 0.556\n",
      "epoch 23 iter 200, loss: 1.374, accuracy: 0.557\n",
      "epoch 24 iter 0, loss: 1.391, accuracy: 0.548\n",
      "epoch 24 iter 200, loss: 1.344, accuracy: 0.568\n",
      "epoch 25 iter 0, loss: 1.371, accuracy: 0.557\n",
      "epoch 25 iter 200, loss: 1.333, accuracy: 0.573\n",
      "epoch 26 iter 0, loss: 1.366, accuracy: 0.561\n",
      "epoch 26 iter 200, loss: 1.354, accuracy: 0.567\n",
      "epoch 27 iter 0, loss: 1.394, accuracy: 0.552\n",
      "epoch 27 iter 200, loss: 1.353, accuracy: 0.567\n",
      "epoch 28 iter 0, loss: 1.383, accuracy: 0.555\n",
      "epoch 28 iter 200, loss: 1.380, accuracy: 0.557\n",
      "epoch 29 iter 0, loss: 1.419, accuracy: 0.542\n",
      "epoch 29 iter 200, loss: 1.376, accuracy: 0.558\n",
      "epoch 30 iter 0, loss: 1.398, accuracy: 0.551\n",
      "epoch 30 iter 200, loss: 1.372, accuracy: 0.563\n",
      "epoch 31 iter 0, loss: 1.381, accuracy: 0.560\n",
      "epoch 31 iter 200, loss: 1.373, accuracy: 0.566\n",
      "epoch 32 iter 0, loss: 1.375, accuracy: 0.563\n",
      "epoch 32 iter 200, loss: 1.377, accuracy: 0.566\n",
      "epoch 33 iter 0, loss: 1.381, accuracy: 0.565\n",
      "epoch 33 iter 200, loss: 1.350, accuracy: 0.571\n",
      "epoch 34 iter 0, loss: 1.419, accuracy: 0.550\n",
      "epoch 34 iter 200, loss: 1.368, accuracy: 0.567\n",
      "epoch 35 iter 0, loss: 1.400, accuracy: 0.558\n",
      "epoch 35 iter 200, loss: 1.371, accuracy: 0.570\n",
      "epoch 36 iter 0, loss: 1.384, accuracy: 0.563\n",
      "epoch 36 iter 200, loss: 1.372, accuracy: 0.571\n",
      "epoch 37 iter 0, loss: 1.398, accuracy: 0.561\n",
      "epoch 37 iter 200, loss: 1.384, accuracy: 0.565\n",
      "epoch 38 iter 0, loss: 1.407, accuracy: 0.558\n",
      "epoch 38 iter 200, loss: 1.364, accuracy: 0.573\n",
      "epoch 39 iter 0, loss: 1.408, accuracy: 0.558\n",
      "epoch 39 iter 200, loss: 1.372, accuracy: 0.572\n",
      "epoch 40 iter 0, loss: 1.416, accuracy: 0.558\n",
      "epoch 40 iter 200, loss: 1.370, accuracy: 0.572\n",
      "epoch 41 iter 0, loss: 1.408, accuracy: 0.561\n",
      "epoch 41 iter 200, loss: 1.366, accuracy: 0.572\n",
      "epoch 42 iter 0, loss: 1.405, accuracy: 0.566\n",
      "epoch 42 iter 200, loss: 1.357, accuracy: 0.580\n",
      "epoch 43 iter 0, loss: 1.390, accuracy: 0.567\n",
      "epoch 43 iter 200, loss: 1.382, accuracy: 0.571\n",
      "epoch 44 iter 0, loss: 1.413, accuracy: 0.563\n",
      "epoch 44 iter 200, loss: 1.401, accuracy: 0.569\n",
      "epoch 45 iter 0, loss: 1.389, accuracy: 0.570\n",
      "epoch 45 iter 200, loss: 1.389, accuracy: 0.578\n",
      "epoch 46 iter 0, loss: 1.388, accuracy: 0.574\n",
      "epoch 46 iter 200, loss: 1.408, accuracy: 0.565\n",
      "epoch 47 iter 0, loss: 1.396, accuracy: 0.573\n",
      "epoch 47 iter 200, loss: 1.421, accuracy: 0.564\n",
      "epoch 48 iter 0, loss: 1.404, accuracy: 0.569\n",
      "epoch 48 iter 200, loss: 1.417, accuracy: 0.565\n",
      "epoch 49 iter 0, loss: 1.389, accuracy: 0.572\n",
      "epoch 49 iter 200, loss: 1.420, accuracy: 0.566\n"
     ]
    }
   ],
   "source": [
    "def cnn_modification(x_):\n",
    "#     raise NotImplemented(\"Add your code here!\")\n",
    "        \n",
    "    ###################################\n",
    "    \n",
    "    conv1 = tf.layers.conv2d(\n",
    "            inputs=x_,\n",
    "            filters=32,  # number of filters\n",
    "            kernel_size=[5, 5],\n",
    "            padding=\"same\",\n",
    "            activation=tf.nn.relu)\n",
    "    \n",
    "    pool1 = tf.layers.max_pooling2d(inputs=conv1, \n",
    "                                    pool_size=[2, 2], \n",
    "                                    strides=16)  # convolution stride\n",
    "    \n",
    "    conv2 = tf.layers.conv2d(\n",
    "            inputs=pool1,\n",
    "            filters=32, # number of filters\n",
    "            kernel_size=[5, 5],\n",
    "            padding=\"same\",\n",
    "            activation=tf.nn.relu)\n",
    "    \n",
    "    pool2 = tf.layers.max_pooling2d(inputs=conv2, \n",
    "                                    pool_size=[2, 2], \n",
    "                                    strides=16)  # convolution stride\n",
    "        \n",
    "    pool_flat = tf.contrib.layers.flatten(pool2, scope='pool2flat')\n",
    "    dense = tf.layers.dense(inputs=pool_flat, units=500, activation=tf.nn.relu)\n",
    "    logits = tf.layers.dense(inputs=dense, units=10)\n",
    "    return logits\n",
    "    \n",
    "    ###################################\n",
    "\n",
    "modified_model_dict = apply_classification_loss(cnn_modification)\n",
    "train_model(modified_model_dict, dataset_generators, epoch_n=50, print_every=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Saving and Reloading Model Weights\n",
    "(25 points)\n",
    "\n",
    "In this section you learn to save the weights of a trained model, and to load the weights of a saved model. This is really useful when we would like to load an already trained model in order to continue training or to fine-tune it. **Often times we save “snapshots” of the trained model as training progresses in case the training is interrupted, or in case we would like to fall back to an earlier model, this is called *snapshot saving*.**\n",
    "\n",
    "### Q2.1 Defining another network\n",
    "Define a network with a slightly different structure in `def cnn_expanded(x_)` below. `cnn_expanded` is an expanded version of `cnn_model`. \n",
    "It should have: \n",
    "- a different size of kernel for the last convolutional layer, \n",
    "- followed by one additional convolutional layer, and \n",
    "- followed by one additional pooling layer.\n",
    "\n",
    "The last fully-connected layer will stay the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the new model (see cnn_map(x_) above for an example)\n",
    "def cnn_expanded(x_):\n",
    "#     raise NotImplemented(\"Add your code here!\")\n",
    "        \n",
    "    ###################################\n",
    "    with tf.variable_scope('conv1'):\n",
    "        conv1 = tf.layers.conv2d(\n",
    "                inputs=x_,\n",
    "                filters=32,  # number of filters\n",
    "                kernel_size=[5, 5],\n",
    "                padding=\"same\",\n",
    "                activation=tf.nn.relu)\n",
    "    \n",
    "    pool1 = tf.layers.max_pooling2d(inputs=conv1, \n",
    "                                    pool_size=[2, 2], \n",
    "                                    strides=2)  # convolution stride\n",
    "    \n",
    "    conv2 = tf.layers.conv2d(\n",
    "            inputs=pool1,\n",
    "            filters=32, # number of filters\n",
    "            kernel_size=[8, 8],\n",
    "            padding=\"same\",\n",
    "            activation=tf.nn.relu)\n",
    "    \n",
    "    pool2 = tf.layers.max_pooling2d(inputs=conv2, \n",
    "                                    pool_size=[2, 2], \n",
    "                                    strides=2)  # convolution stride\n",
    "    \n",
    "    conv3 = tf.layers.conv2d(\n",
    "            inputs=pool2,\n",
    "            filters=32,  # number of filters\n",
    "            kernel_size=[5, 5],\n",
    "            padding=\"same\",\n",
    "            activation=tf.nn.relu)\n",
    "    \n",
    "    pool3 = tf.layers.max_pooling2d(inputs=conv2, \n",
    "                                    pool_size=[2, 2], \n",
    "                                    strides=2)  # convolution stride\n",
    "    \n",
    "    pool_flat = tf.contrib.layers.flatten(pool3, scope='pool3flat')\n",
    "    dense = tf.layers.dense(inputs=pool_flat, units=500, activation=tf.nn.relu)\n",
    "    logits = tf.layers.dense(inputs=dense, units=10)\n",
    "    return logits\n",
    "    \n",
    "    ###################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2.2 Saving and Loading Weights\n",
    "`new_train_model()` below has two additional parameters `save_model=False, load_model=False` than `train_model` defined previously. Modify `new_train_model()` such that it would \n",
    "- save weights after the training is complete if `save_model` is `True`, and\n",
    "- load weights on start-up before training if `load_model` is `True`.\n",
    "\n",
    "*Hint:*  `tf.train.Saver()`.\n",
    "\n",
    "Note: if you are unable to load weights into `cnn_expanded` network, use `cnn_map` in order to continue the assingment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Modify this:\n",
    "def new_train_model(model_dict, dataset_generators, epoch_n, print_every,\n",
    "                    save_model=False, load_model=False):\n",
    "    with model_dict['graph'].as_default(), tf.Session() as sess:\n",
    "        to_save_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'conv1')\n",
    "        model_saver = tf.train.Saver(to_save_vars)\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        # Model Loading Logic\n",
    "        # ----------------------------------\n",
    "        if load_model:           \n",
    "            # Create Model Loader\n",
    "            model_saver.restore(sess, './model.ckpt')\n",
    "            print(\"Model Loaded!!!\")\n",
    "        # ----------------------------------\n",
    "            \n",
    "        for epoch_i in range(epoch_n):\n",
    "            for iter_i, data_batch in enumerate(dataset_generators['train']):\n",
    "                train_feed_dict = dict(zip(model_dict['inputs'], data_batch))\n",
    "                sess.run(model_dict['train_op'], feed_dict=train_feed_dict)\n",
    "                \n",
    "                if iter_i % print_every == 0:\n",
    "                    collect_arr = []\n",
    "                    for test_batch in dataset_generators['test']:\n",
    "                        test_feed_dict = dict(zip(model_dict['inputs'], test_batch))\n",
    "                        to_compute = [model_dict['loss'], model_dict['accuracy']]\n",
    "                        collect_arr.append(sess.run(to_compute, test_feed_dict))\n",
    "                    averages = np.mean(collect_arr, axis=0)\n",
    "                    fmt = (epoch_i, iter_i, ) + tuple(averages)\n",
    "                    print('iteration {:d} {:d}\\t loss: {:.3f}, '\n",
    "                          'accuracy: {:.3f}'.format(*fmt))\n",
    "\n",
    "        # Model Saving Logic\n",
    "        # ----------------------------------\n",
    "        if save_model:\n",
    "            # Create Model Saver\n",
    "            model_saver.save(sess, './model.ckpt')\n",
    "            print(\"Model Saved!!!\")\n",
    "        # ----------------------------------\n",
    "\n",
    "def test_saving():\n",
    "    ### Hint: call the saver like this: tf.train.Saver(var_list)\n",
    "    ### where var_list is a list of TF variables you want to save (from conv1 layer)\n",
    "    model_dict = apply_classification_loss(cnn_map)\n",
    "    new_train_model(model_dict, dataset_generators, epoch_n=50, print_every=200, save_model=True)\n",
    "    ### Hint: call the saver like this: tf.train.Saver(var_list)\n",
    "    ### where var_list is a list of TF variables you want to load from the checkpoint (for conv1 layer)\n",
    "    cnn_expanded_dict = apply_classification_loss(cnn_expanded)\n",
    "    new_train_model(cnn_expanded_dict, dataset_generators, epoch_n=20, print_every=200, load_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 0\t loss: 86.894, accuracy: 0.111\n",
      "iteration 0 200\t loss: 2.115, accuracy: 0.246\n",
      "iteration 1 0\t loss: 2.064, accuracy: 0.252\n",
      "iteration 1 200\t loss: 1.170, accuracy: 0.644\n",
      "iteration 2 0\t loss: 1.066, accuracy: 0.676\n",
      "iteration 2 200\t loss: 0.955, accuracy: 0.716\n",
      "iteration 3 0\t loss: 0.922, accuracy: 0.727\n",
      "iteration 3 200\t loss: 0.839, accuracy: 0.758\n",
      "iteration 4 0\t loss: 0.830, accuracy: 0.760\n",
      "iteration 4 200\t loss: 0.811, accuracy: 0.769\n",
      "iteration 5 0\t loss: 0.829, accuracy: 0.769\n",
      "iteration 5 200\t loss: 0.827, accuracy: 0.775\n",
      "iteration 6 0\t loss: 0.823, accuracy: 0.776\n",
      "iteration 6 200\t loss: 0.817, accuracy: 0.784\n",
      "iteration 7 0\t loss: 0.795, accuracy: 0.792\n",
      "iteration 7 200\t loss: 0.847, accuracy: 0.783\n",
      "iteration 8 0\t loss: 0.788, accuracy: 0.805\n",
      "iteration 8 200\t loss: 0.853, accuracy: 0.790\n",
      "iteration 9 0\t loss: 0.761, accuracy: 0.811\n",
      "iteration 9 200\t loss: 0.803, accuracy: 0.806\n",
      "iteration 10 0\t loss: 0.765, accuracy: 0.824\n",
      "iteration 10 200\t loss: 0.784, accuracy: 0.825\n",
      "iteration 11 0\t loss: 0.794, accuracy: 0.830\n",
      "iteration 11 200\t loss: 0.807, accuracy: 0.827\n",
      "iteration 12 0\t loss: 0.935, accuracy: 0.810\n",
      "iteration 12 200\t loss: 0.894, accuracy: 0.829\n",
      "iteration 13 0\t loss: 0.886, accuracy: 0.829\n",
      "iteration 13 200\t loss: 0.908, accuracy: 0.829\n",
      "iteration 14 0\t loss: 0.962, accuracy: 0.827\n",
      "iteration 14 200\t loss: 0.965, accuracy: 0.828\n",
      "iteration 15 0\t loss: 0.966, accuracy: 0.831\n",
      "iteration 15 200\t loss: 1.026, accuracy: 0.835\n",
      "iteration 16 0\t loss: 1.031, accuracy: 0.828\n",
      "iteration 16 200\t loss: 1.075, accuracy: 0.828\n",
      "iteration 17 0\t loss: 1.118, accuracy: 0.826\n",
      "iteration 17 200\t loss: 1.196, accuracy: 0.826\n",
      "iteration 18 0\t loss: 1.055, accuracy: 0.838\n",
      "iteration 18 200\t loss: 1.191, accuracy: 0.830\n",
      "iteration 19 0\t loss: 1.208, accuracy: 0.831\n",
      "iteration 19 200\t loss: 1.229, accuracy: 0.833\n",
      "iteration 20 0\t loss: 1.253, accuracy: 0.833\n",
      "iteration 20 200\t loss: 1.243, accuracy: 0.833\n",
      "iteration 21 0\t loss: 1.366, accuracy: 0.828\n",
      "iteration 21 200\t loss: 1.356, accuracy: 0.828\n",
      "iteration 22 0\t loss: 1.414, accuracy: 0.830\n",
      "iteration 22 200\t loss: 1.498, accuracy: 0.832\n",
      "iteration 23 0\t loss: 1.588, accuracy: 0.829\n",
      "iteration 23 200\t loss: 1.500, accuracy: 0.836\n",
      "iteration 24 0\t loss: 1.583, accuracy: 0.828\n",
      "iteration 24 200\t loss: 1.640, accuracy: 0.825\n",
      "iteration 25 0\t loss: 1.632, accuracy: 0.824\n",
      "iteration 25 200\t loss: 1.906, accuracy: 0.821\n",
      "iteration 26 0\t loss: 1.797, accuracy: 0.824\n",
      "iteration 26 200\t loss: 1.730, accuracy: 0.818\n",
      "iteration 27 0\t loss: 1.836, accuracy: 0.830\n",
      "iteration 27 200\t loss: 1.743, accuracy: 0.836\n",
      "iteration 28 0\t loss: 2.058, accuracy: 0.832\n",
      "iteration 28 200\t loss: 1.783, accuracy: 0.842\n",
      "iteration 29 0\t loss: 1.887, accuracy: 0.832\n",
      "iteration 29 200\t loss: 1.957, accuracy: 0.838\n",
      "iteration 30 0\t loss: 2.050, accuracy: 0.830\n",
      "iteration 30 200\t loss: 1.927, accuracy: 0.837\n",
      "iteration 31 0\t loss: 2.177, accuracy: 0.826\n",
      "iteration 31 200\t loss: 1.963, accuracy: 0.832\n",
      "iteration 32 0\t loss: 2.186, accuracy: 0.827\n",
      "iteration 32 200\t loss: 2.049, accuracy: 0.837\n",
      "iteration 33 0\t loss: 2.339, accuracy: 0.836\n",
      "iteration 33 200\t loss: 2.009, accuracy: 0.846\n",
      "iteration 34 0\t loss: 2.487, accuracy: 0.832\n",
      "iteration 34 200\t loss: 2.309, accuracy: 0.840\n",
      "iteration 35 0\t loss: 2.419, accuracy: 0.839\n",
      "iteration 35 200\t loss: 2.239, accuracy: 0.847\n",
      "iteration 36 0\t loss: 2.479, accuracy: 0.833\n",
      "iteration 36 200\t loss: 2.238, accuracy: 0.848\n",
      "iteration 37 0\t loss: 2.596, accuracy: 0.829\n",
      "iteration 37 200\t loss: 2.515, accuracy: 0.841\n",
      "iteration 38 0\t loss: 2.545, accuracy: 0.835\n",
      "iteration 38 200\t loss: 2.672, accuracy: 0.838\n",
      "iteration 39 0\t loss: 2.584, accuracy: 0.836\n",
      "iteration 39 200\t loss: 2.539, accuracy: 0.838\n",
      "iteration 40 0\t loss: 2.649, accuracy: 0.837\n",
      "iteration 40 200\t loss: 2.626, accuracy: 0.843\n",
      "iteration 41 0\t loss: 2.811, accuracy: 0.831\n",
      "iteration 41 200\t loss: 2.753, accuracy: 0.845\n",
      "iteration 42 0\t loss: 2.984, accuracy: 0.832\n",
      "iteration 42 200\t loss: 3.123, accuracy: 0.841\n",
      "iteration 43 0\t loss: 3.093, accuracy: 0.835\n",
      "iteration 43 200\t loss: 3.043, accuracy: 0.841\n",
      "iteration 44 0\t loss: 2.989, accuracy: 0.835\n",
      "iteration 44 200\t loss: 2.882, accuracy: 0.841\n",
      "iteration 45 0\t loss: 2.913, accuracy: 0.840\n",
      "iteration 45 200\t loss: 3.569, accuracy: 0.832\n",
      "iteration 46 0\t loss: 3.284, accuracy: 0.831\n",
      "iteration 46 200\t loss: 2.941, accuracy: 0.842\n",
      "iteration 47 0\t loss: 3.342, accuracy: 0.832\n",
      "iteration 47 200\t loss: 3.324, accuracy: 0.841\n",
      "iteration 48 0\t loss: 3.336, accuracy: 0.837\n",
      "iteration 48 200\t loss: 3.236, accuracy: 0.838\n",
      "iteration 49 0\t loss: 3.420, accuracy: 0.832\n",
      "iteration 49 200\t loss: 3.208, accuracy: 0.847\n",
      "Model Saved!!!\n",
      "Model Loaded!!!\n",
      "iteration 0 0\t loss: 136.382, accuracy: 0.196\n",
      "iteration 0 200\t loss: 2.193, accuracy: 0.197\n",
      "iteration 1 0\t loss: 2.141, accuracy: 0.213\n",
      "iteration 1 200\t loss: 1.238, accuracy: 0.616\n",
      "iteration 2 0\t loss: 1.055, accuracy: 0.688\n",
      "iteration 2 200\t loss: 0.913, accuracy: 0.729\n",
      "iteration 3 0\t loss: 0.888, accuracy: 0.741\n",
      "iteration 3 200\t loss: 0.837, accuracy: 0.755\n",
      "iteration 4 0\t loss: 0.828, accuracy: 0.762\n",
      "iteration 4 200\t loss: 0.847, accuracy: 0.750\n",
      "iteration 5 0\t loss: 0.810, accuracy: 0.772\n",
      "iteration 5 200\t loss: 0.824, accuracy: 0.761\n",
      "iteration 6 0\t loss: 0.809, accuracy: 0.772\n",
      "iteration 6 200\t loss: 0.819, accuracy: 0.767\n",
      "iteration 7 0\t loss: 0.868, accuracy: 0.762\n",
      "iteration 7 200\t loss: 0.849, accuracy: 0.759\n",
      "iteration 8 0\t loss: 0.825, accuracy: 0.775\n",
      "iteration 8 200\t loss: 0.855, accuracy: 0.759\n",
      "iteration 9 0\t loss: 0.807, accuracy: 0.780\n",
      "iteration 9 200\t loss: 0.843, accuracy: 0.770\n",
      "iteration 10 0\t loss: 0.814, accuracy: 0.781\n",
      "iteration 10 200\t loss: 0.818, accuracy: 0.783\n",
      "iteration 11 0\t loss: 0.841, accuracy: 0.778\n",
      "iteration 11 200\t loss: 0.854, accuracy: 0.784\n",
      "iteration 12 0\t loss: 0.860, accuracy: 0.778\n",
      "iteration 12 200\t loss: 0.896, accuracy: 0.776\n",
      "iteration 13 0\t loss: 0.919, accuracy: 0.774\n",
      "iteration 13 200\t loss: 0.880, accuracy: 0.788\n",
      "iteration 14 0\t loss: 0.911, accuracy: 0.783\n",
      "iteration 14 200\t loss: 0.857, accuracy: 0.792\n",
      "iteration 15 0\t loss: 1.005, accuracy: 0.771\n",
      "iteration 15 200\t loss: 0.926, accuracy: 0.787\n",
      "iteration 16 0\t loss: 0.942, accuracy: 0.778\n",
      "iteration 16 200\t loss: 1.026, accuracy: 0.774\n",
      "iteration 17 0\t loss: 1.110, accuracy: 0.775\n",
      "iteration 17 200\t loss: 1.001, accuracy: 0.785\n",
      "iteration 18 0\t loss: 1.192, accuracy: 0.776\n",
      "iteration 18 200\t loss: 0.968, accuracy: 0.797\n",
      "iteration 19 0\t loss: 1.266, accuracy: 0.759\n",
      "iteration 19 200\t loss: 0.996, accuracy: 0.800\n"
     ]
    }
   ],
   "source": [
    "test_saving()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Fine-tuning a Pre-trained Network on CIFAR-10\n",
    "(20 points)\n",
    "\n",
    "[CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) is another popular benchmark for image classification.\n",
    "We provide you with modified version of the file cifar10.py from [https://github.com/Hvass-Labs/TensorFlow-Tutorials](https://github.com/Hvass-Labs/TensorFlow-Tutorials).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import read_cifar10 as cf10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also provide a generator for the CIFAR-10 Dataset, yielding the next batch every time next is invoked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@read_data.restartable\n",
    "def cifar10_dataset_generator(dataset_name, batch_size, restrict_size=1000):\n",
    "    assert dataset_name in ['train', 'test']\n",
    "    assert batch_size > 0 or batch_size == -1  # -1 for entire dataset\n",
    "    \n",
    "    X_all_unrestricted, y_all = (cf10.load_training_data() if dataset_name == 'train'\n",
    "                                 else cf10.load_test_data())\n",
    "    \n",
    "    actual_restrict_size = restrict_size if dataset_name == 'train' else int(1e10)\n",
    "    X_all = X_all_unrestricted[:actual_restrict_size]\n",
    "    data_len = X_all.shape[0]\n",
    "    batch_size = batch_size if batch_size > 0 else data_len\n",
    "    \n",
    "    X_all_padded = np.concatenate([X_all, X_all[:batch_size]], axis=0)\n",
    "    y_all_padded = np.concatenate([y_all, y_all[:batch_size]], axis=0)\n",
    "    \n",
    "    for slice_i in range(math.ceil(data_len / batch_size)):\n",
    "        idx = slice_i * batch_size\n",
    "        #X_batch = X_all_padded[idx:idx + batch_size]\n",
    "        X_batch = X_all_padded[idx:idx + batch_size]*255  # bugfix: thanks Zezhou Sun!\n",
    "        y_batch = np.ravel(y_all_padded[idx:idx + batch_size])\n",
    "        yield X_batch.astype(np.uint8), y_batch.astype(np.uint8)\n",
    "\n",
    "cifar10_dataset_generators = {\n",
    "    'train': cifar10_dataset_generator('train', 1000),\n",
    "    'test': cifar10_dataset_generator('test', -1)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3.1 Fine-tuning\n",
    "Let's fine-tune SVHN net on **1000 examples** from CIFAR-10. \n",
    "Compare test accuracies of the following scenarios: \n",
    "  - Training `cnn_map` from scratch on the 1000 CIFAR-10 examples\n",
    "  - Fine-tuning SVHN net (`cnn_map` trained on SVHN dataset) on 1000 exampes from CIFAR-10. Use `new_train_model()` defined above to load SVHN net weights, but train on the CIFAR-10 examples.\n",
    "  \n",
    "**Important:** please do not change the `restrict_size=1000` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnn_expanded_dict = apply_classification_loss(cnn_expanded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Download progress: 100.0%\n",
      "Download finished. Extracting files.\n",
      "Done.\n",
      "iteration 0 0\t loss: 76.687, accuracy: 0.093\n",
      "iteration 1 0\t loss: 77.076, accuracy: 0.129\n",
      "iteration 2 0\t loss: 44.266, accuracy: 0.102\n",
      "iteration 3 0\t loss: 26.613, accuracy: 0.105\n",
      "iteration 4 0\t loss: 11.161, accuracy: 0.111\n",
      "iteration 5 0\t loss: 4.251, accuracy: 0.100\n",
      "iteration 6 0\t loss: 2.634, accuracy: 0.100\n",
      "iteration 7 0\t loss: 2.346, accuracy: 0.105\n",
      "iteration 8 0\t loss: 2.307, accuracy: 0.131\n",
      "iteration 9 0\t loss: 2.304, accuracy: 0.129\n",
      "iteration 10 0\t loss: 2.304, accuracy: 0.118\n",
      "iteration 11 0\t loss: 2.304, accuracy: 0.116\n",
      "iteration 12 0\t loss: 2.303, accuracy: 0.114\n",
      "iteration 13 0\t loss: 2.303, accuracy: 0.114\n",
      "iteration 14 0\t loss: 2.302, accuracy: 0.115\n",
      "iteration 15 0\t loss: 2.302, accuracy: 0.115\n",
      "iteration 16 0\t loss: 2.301, accuracy: 0.116\n",
      "iteration 17 0\t loss: 2.301, accuracy: 0.116\n",
      "iteration 18 0\t loss: 2.300, accuracy: 0.115\n",
      "iteration 19 0\t loss: 2.298, accuracy: 0.117\n",
      "iteration 20 0\t loss: 2.297, accuracy: 0.120\n",
      "iteration 21 0\t loss: 2.296, accuracy: 0.126\n",
      "iteration 22 0\t loss: 2.294, accuracy: 0.126\n",
      "iteration 23 0\t loss: 2.292, accuracy: 0.127\n",
      "iteration 24 0\t loss: 2.291, accuracy: 0.132\n",
      "iteration 25 0\t loss: 2.290, accuracy: 0.140\n",
      "iteration 26 0\t loss: 2.291, accuracy: 0.146\n",
      "iteration 27 0\t loss: 2.295, accuracy: 0.145\n",
      "iteration 28 0\t loss: 2.295, accuracy: 0.149\n",
      "iteration 29 0\t loss: 2.288, accuracy: 0.151\n",
      "iteration 30 0\t loss: 2.282, accuracy: 0.154\n",
      "iteration 31 0\t loss: 2.279, accuracy: 0.154\n",
      "iteration 32 0\t loss: 2.276, accuracy: 0.162\n",
      "iteration 33 0\t loss: 2.277, accuracy: 0.170\n",
      "iteration 34 0\t loss: 2.279, accuracy: 0.175\n",
      "iteration 35 0\t loss: 2.266, accuracy: 0.182\n",
      "iteration 36 0\t loss: 2.259, accuracy: 0.184\n",
      "iteration 37 0\t loss: 2.260, accuracy: 0.185\n",
      "iteration 38 0\t loss: 2.272, accuracy: 0.185\n",
      "iteration 39 0\t loss: 2.279, accuracy: 0.192\n",
      "iteration 40 0\t loss: 2.278, accuracy: 0.193\n",
      "iteration 41 0\t loss: 2.279, accuracy: 0.194\n",
      "iteration 42 0\t loss: 2.274, accuracy: 0.196\n",
      "iteration 43 0\t loss: 2.276, accuracy: 0.198\n",
      "iteration 44 0\t loss: 2.271, accuracy: 0.197\n",
      "iteration 45 0\t loss: 2.267, accuracy: 0.201\n",
      "iteration 46 0\t loss: 2.273, accuracy: 0.201\n",
      "iteration 47 0\t loss: 2.286, accuracy: 0.204\n",
      "iteration 48 0\t loss: 2.293, accuracy: 0.206\n",
      "iteration 49 0\t loss: 2.283, accuracy: 0.209\n"
     ]
    }
   ],
   "source": [
    "## train a model from scratch\n",
    "new_train_model(cnn_expanded_dict, cifar10_dataset_generators, epoch_n=50, \n",
    "                print_every=10, save_model=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Loaded!!!\n",
      "iteration 0 0\t loss: 113.406, accuracy: 0.140\n",
      "iteration 1 0\t loss: 89.570, accuracy: 0.119\n",
      "iteration 2 0\t loss: 57.906, accuracy: 0.144\n",
      "iteration 3 0\t loss: 32.672, accuracy: 0.135\n",
      "iteration 4 0\t loss: 17.652, accuracy: 0.120\n",
      "iteration 5 0\t loss: 9.062, accuracy: 0.112\n",
      "iteration 6 0\t loss: 5.299, accuracy: 0.102\n",
      "iteration 7 0\t loss: 3.591, accuracy: 0.093\n",
      "iteration 8 0\t loss: 2.747, accuracy: 0.093\n",
      "iteration 9 0\t loss: 2.425, accuracy: 0.109\n",
      "iteration 10 0\t loss: 2.347, accuracy: 0.127\n",
      "iteration 11 0\t loss: 2.325, accuracy: 0.130\n",
      "iteration 12 0\t loss: 2.316, accuracy: 0.130\n",
      "iteration 13 0\t loss: 2.311, accuracy: 0.111\n",
      "iteration 14 0\t loss: 2.308, accuracy: 0.105\n",
      "iteration 15 0\t loss: 2.306, accuracy: 0.105\n",
      "iteration 16 0\t loss: 2.305, accuracy: 0.102\n",
      "iteration 17 0\t loss: 2.304, accuracy: 0.101\n",
      "iteration 18 0\t loss: 2.304, accuracy: 0.101\n",
      "iteration 19 0\t loss: 2.304, accuracy: 0.101\n",
      "iteration 20 0\t loss: 2.304, accuracy: 0.101\n",
      "iteration 21 0\t loss: 2.303, accuracy: 0.100\n",
      "iteration 22 0\t loss: 2.303, accuracy: 0.099\n",
      "iteration 23 0\t loss: 2.303, accuracy: 0.100\n",
      "iteration 24 0\t loss: 2.303, accuracy: 0.100\n",
      "iteration 25 0\t loss: 2.303, accuracy: 0.100\n",
      "iteration 26 0\t loss: 2.303, accuracy: 0.100\n",
      "iteration 27 0\t loss: 2.303, accuracy: 0.100\n",
      "iteration 28 0\t loss: 2.303, accuracy: 0.100\n",
      "iteration 29 0\t loss: 2.303, accuracy: 0.100\n",
      "iteration 30 0\t loss: 2.303, accuracy: 0.100\n",
      "iteration 31 0\t loss: 2.303, accuracy: 0.100\n",
      "iteration 32 0\t loss: 2.303, accuracy: 0.100\n",
      "iteration 33 0\t loss: 2.303, accuracy: 0.100\n",
      "iteration 34 0\t loss: 2.303, accuracy: 0.100\n",
      "iteration 35 0\t loss: 2.303, accuracy: 0.100\n",
      "iteration 36 0\t loss: 2.303, accuracy: 0.099\n",
      "iteration 37 0\t loss: 2.303, accuracy: 0.100\n",
      "iteration 38 0\t loss: 2.303, accuracy: 0.100\n",
      "iteration 39 0\t loss: 2.303, accuracy: 0.100\n",
      "iteration 40 0\t loss: 2.303, accuracy: 0.100\n",
      "iteration 41 0\t loss: 2.303, accuracy: 0.100\n",
      "iteration 42 0\t loss: 2.303, accuracy: 0.100\n",
      "iteration 43 0\t loss: 2.303, accuracy: 0.100\n",
      "iteration 44 0\t loss: 2.303, accuracy: 0.100\n",
      "iteration 45 0\t loss: 2.303, accuracy: 0.100\n",
      "iteration 46 0\t loss: 2.303, accuracy: 0.100\n",
      "iteration 47 0\t loss: 2.303, accuracy: 0.100\n",
      "iteration 48 0\t loss: 2.303, accuracy: 0.100\n",
      "iteration 49 0\t loss: 2.304, accuracy: 0.100\n"
     ]
    }
   ],
   "source": [
    "## fine-tuning SVHN Net using Cifar-10 weights saved in Q2\n",
    "new_train_model(cnn_expanded_dict, cifar10_dataset_generators, epoch_n=50, \n",
    "                print_every=10, load_model=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: TensorBoard\n",
    "(30 points)\n",
    "\n",
    "[TensorBoard](https://www.tensorflow.org/get_started/summaries_and_tensorboard) is a very helpful tool for visualization of neural networks. \n",
    "\n",
    "### Q4.1 Plotting\n",
    "Present at least one visualization for each of the following:\n",
    "  - Filters\n",
    "  - Loss\n",
    "  - Accuracy\n",
    "\n",
    "Modify code you have wrote above to also have summary writers. To  run tensorboard, the command is `tensorboard --logdir=path/to/your/log/directory`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For this part, I re-implemented the cnn_expanded model, using tf.nn.conv2d() completely so that I can extract all variables I wanted to visualize, for example the filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 iter 0, loss: 425.365, accuracy: 0.136\n",
      "epoch 0 iter 200, loss: 2.204, accuracy: 0.229\n",
      "epoch 1 iter 0, loss: 2.144, accuracy: 0.257\n",
      "epoch 1 iter 200, loss: 1.893, accuracy: 0.372\n",
      "epoch 2 iter 0, loss: 1.825, accuracy: 0.398\n",
      "epoch 2 iter 200, loss: 1.491, accuracy: 0.535\n",
      "epoch 3 iter 0, loss: 1.396, accuracy: 0.577\n",
      "epoch 3 iter 200, loss: 1.257, accuracy: 0.625\n",
      "epoch 4 iter 0, loss: 1.203, accuracy: 0.643\n",
      "epoch 4 iter 200, loss: 1.107, accuracy: 0.672\n",
      "epoch 5 iter 0, loss: 1.072, accuracy: 0.686\n",
      "epoch 5 iter 200, loss: 1.079, accuracy: 0.687\n",
      "epoch 6 iter 0, loss: 0.991, accuracy: 0.713\n",
      "epoch 6 iter 200, loss: 0.941, accuracy: 0.731\n",
      "epoch 7 iter 0, loss: 0.914, accuracy: 0.740\n",
      "epoch 7 iter 200, loss: 0.879, accuracy: 0.754\n",
      "epoch 8 iter 0, loss: 0.855, accuracy: 0.760\n",
      "epoch 8 iter 200, loss: 0.823, accuracy: 0.770\n",
      "epoch 9 iter 0, loss: 0.818, accuracy: 0.770\n",
      "epoch 9 iter 200, loss: 0.823, accuracy: 0.767\n",
      "epoch 10 iter 0, loss: 0.824, accuracy: 0.774\n",
      "epoch 10 iter 200, loss: 0.799, accuracy: 0.778\n",
      "epoch 11 iter 0, loss: 0.774, accuracy: 0.792\n",
      "epoch 11 iter 200, loss: 0.795, accuracy: 0.786\n",
      "epoch 12 iter 0, loss: 0.781, accuracy: 0.793\n",
      "epoch 12 iter 200, loss: 0.799, accuracy: 0.791\n",
      "epoch 13 iter 0, loss: 0.797, accuracy: 0.794\n",
      "epoch 13 iter 200, loss: 0.794, accuracy: 0.799\n",
      "epoch 14 iter 0, loss: 0.771, accuracy: 0.805\n",
      "epoch 14 iter 200, loss: 0.770, accuracy: 0.804\n",
      "epoch 15 iter 0, loss: 0.802, accuracy: 0.808\n",
      "epoch 15 iter 200, loss: 0.776, accuracy: 0.806\n",
      "epoch 16 iter 0, loss: 0.856, accuracy: 0.801\n",
      "epoch 16 iter 200, loss: 0.822, accuracy: 0.797\n",
      "epoch 17 iter 0, loss: 0.893, accuracy: 0.789\n",
      "epoch 17 iter 200, loss: 0.833, accuracy: 0.802\n",
      "epoch 18 iter 0, loss: 0.922, accuracy: 0.794\n",
      "epoch 18 iter 200, loss: 0.780, accuracy: 0.820\n",
      "epoch 19 iter 0, loss: 0.872, accuracy: 0.807\n",
      "epoch 19 iter 200, loss: 0.785, accuracy: 0.818\n",
      "epoch 20 iter 0, loss: 0.839, accuracy: 0.821\n",
      "epoch 20 iter 200, loss: 0.915, accuracy: 0.805\n",
      "epoch 21 iter 0, loss: 0.890, accuracy: 0.815\n",
      "epoch 21 iter 200, loss: 0.901, accuracy: 0.812\n",
      "epoch 22 iter 0, loss: 0.899, accuracy: 0.822\n",
      "epoch 22 iter 200, loss: 0.878, accuracy: 0.813\n",
      "epoch 23 iter 0, loss: 0.921, accuracy: 0.818\n",
      "epoch 23 iter 200, loss: 0.864, accuracy: 0.820\n",
      "epoch 24 iter 0, loss: 0.945, accuracy: 0.814\n",
      "epoch 24 iter 200, loss: 0.871, accuracy: 0.831\n",
      "epoch 25 iter 0, loss: 0.956, accuracy: 0.812\n",
      "epoch 25 iter 200, loss: 0.868, accuracy: 0.827\n",
      "epoch 26 iter 0, loss: 0.925, accuracy: 0.822\n",
      "epoch 26 iter 200, loss: 0.900, accuracy: 0.829\n",
      "epoch 27 iter 0, loss: 0.974, accuracy: 0.823\n",
      "epoch 27 iter 200, loss: 0.905, accuracy: 0.827\n",
      "epoch 28 iter 0, loss: 0.931, accuracy: 0.827\n",
      "epoch 28 iter 200, loss: 0.908, accuracy: 0.831\n",
      "epoch 29 iter 0, loss: 0.898, accuracy: 0.833\n",
      "epoch 29 iter 200, loss: 0.933, accuracy: 0.839\n",
      "epoch 30 iter 0, loss: 0.941, accuracy: 0.837\n",
      "epoch 30 iter 200, loss: 0.940, accuracy: 0.842\n",
      "epoch 31 iter 0, loss: 0.921, accuracy: 0.845\n",
      "epoch 31 iter 200, loss: 0.974, accuracy: 0.833\n",
      "epoch 32 iter 0, loss: 0.973, accuracy: 0.839\n",
      "epoch 32 iter 200, loss: 0.991, accuracy: 0.839\n",
      "epoch 33 iter 0, loss: 0.937, accuracy: 0.836\n",
      "epoch 33 iter 200, loss: 1.008, accuracy: 0.834\n",
      "epoch 34 iter 0, loss: 1.054, accuracy: 0.832\n",
      "epoch 34 iter 200, loss: 0.995, accuracy: 0.841\n",
      "epoch 35 iter 0, loss: 1.046, accuracy: 0.839\n",
      "epoch 35 iter 200, loss: 1.008, accuracy: 0.841\n",
      "epoch 36 iter 0, loss: 1.136, accuracy: 0.827\n",
      "epoch 36 iter 200, loss: 1.044, accuracy: 0.842\n",
      "epoch 37 iter 0, loss: 1.075, accuracy: 0.832\n",
      "epoch 37 iter 200, loss: 1.146, accuracy: 0.840\n",
      "epoch 38 iter 0, loss: 1.148, accuracy: 0.832\n",
      "epoch 38 iter 200, loss: 1.168, accuracy: 0.835\n",
      "epoch 39 iter 0, loss: 1.209, accuracy: 0.835\n",
      "epoch 39 iter 200, loss: 1.171, accuracy: 0.839\n",
      "epoch 40 iter 0, loss: 1.244, accuracy: 0.832\n",
      "epoch 40 iter 200, loss: 1.279, accuracy: 0.835\n",
      "epoch 41 iter 0, loss: 1.289, accuracy: 0.839\n",
      "epoch 41 iter 200, loss: 1.229, accuracy: 0.839\n",
      "epoch 42 iter 0, loss: 1.246, accuracy: 0.835\n",
      "epoch 42 iter 200, loss: 1.238, accuracy: 0.844\n",
      "epoch 43 iter 0, loss: 1.362, accuracy: 0.836\n",
      "epoch 43 iter 200, loss: 1.191, accuracy: 0.835\n",
      "epoch 44 iter 0, loss: 1.301, accuracy: 0.843\n",
      "epoch 44 iter 200, loss: 1.381, accuracy: 0.844\n",
      "epoch 45 iter 0, loss: 1.297, accuracy: 0.850\n",
      "epoch 45 iter 200, loss: 1.396, accuracy: 0.851\n",
      "epoch 46 iter 0, loss: 1.386, accuracy: 0.839\n",
      "epoch 46 iter 200, loss: 1.308, accuracy: 0.853\n",
      "epoch 47 iter 0, loss: 1.318, accuracy: 0.848\n",
      "epoch 47 iter 200, loss: 1.366, accuracy: 0.844\n",
      "epoch 48 iter 0, loss: 1.386, accuracy: 0.850\n",
      "epoch 48 iter 200, loss: 1.615, accuracy: 0.846\n",
      "epoch 49 iter 0, loss: 1.485, accuracy: 0.843\n",
      "epoch 49 iter 200, loss: 1.775, accuracy: 0.848\n"
     ]
    }
   ],
   "source": [
    "# Filter, loss, and accuracy visualizations\n",
    "import copy\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.io\n",
    "import tensorflow as tf\n",
    "\n",
    "from six.moves import range\n",
    "\n",
    "import read_data\n",
    "\n",
    "# SVNH Date Generator\n",
    "@read_data.restartable\n",
    "def svhn_dataset_generator(dataset_name, batch_size):\n",
    "    assert dataset_name in ['train', 'test']\n",
    "    assert batch_size > 0 or batch_size == -1  # -1 for entire dataset\n",
    "\n",
    "    path = './svhn_mat/'  # path to the SVHN dataset you will download in Q1.1\n",
    "    file_name = '%s_32x32.mat' % dataset_name\n",
    "    file_dict = scipy.io.loadmat(os.path.join(path, file_name))\n",
    "    X_all = file_dict['X'].transpose((3, 0, 1, 2))\n",
    "    y_all = file_dict['y']\n",
    "    data_len = X_all.shape[0]\n",
    "    batch_size = batch_size if batch_size > 0 else data_len\n",
    "\n",
    "    X_all_padded = np.concatenate([X_all, X_all[:batch_size]], axis=0)\n",
    "    y_all_padded = np.concatenate([y_all, y_all[:batch_size]], axis=0)\n",
    "    y_all_padded[y_all_padded == 10] = 0\n",
    "\n",
    "    for slice_i in range(int(math.ceil(data_len / batch_size))):\n",
    "        idx = slice_i * batch_size\n",
    "        X_batch = X_all_padded[idx:idx + batch_size]\n",
    "        y_batch = np.ravel(y_all_padded[idx:idx + batch_size])\n",
    "        yield X_batch, y_batch\n",
    "\n",
    "# Helper function to Visualize conv. features as an image\n",
    "# Refer to: https://gist.github.com/kukuruza/03731dc494603ceab0c5\n",
    "def put_kernels_on_grid (kernel, grid_Y, grid_X, pad = 1):\n",
    "    '''Visualize conv. features as an image (mostly for the 1st layer).\n",
    "    Place kernel into a grid, with some paddings between adjacent filters.\n",
    "\n",
    "    Args:\n",
    "      kernel:            tensor of shape [Y, X, NumChannels, NumKernels]\n",
    "      (grid_Y, grid_X):  shape of the grid. Require: NumKernels == grid_Y * grid_X\n",
    "                           User is responsible of how to break into two multiples.\n",
    "      pad:               number of black pixels around each filter (between them)\n",
    "\n",
    "    Return:\n",
    "      Tensor of shape [(Y+2*pad)*grid_Y, (X+2*pad)*grid_X, NumChannels, 1].\n",
    "    '''\n",
    "    x_min = tf.reduce_min(kernel)\n",
    "    x_max = tf.reduce_max(kernel)\n",
    "\n",
    "    kernel1 = (kernel - x_min) / (x_max - x_min)\n",
    "\n",
    "    # pad X and Y\n",
    "    x1 = tf.pad(kernel1, tf.constant( [[pad,pad],[pad, pad],[0,0],[0,0]] ), mode = 'CONSTANT')\n",
    "\n",
    "    # X and Y dimensions, w.r.t. padding\n",
    "    Y = kernel1.get_shape()[0] + 2 * pad\n",
    "    X = kernel1.get_shape()[1] + 2 * pad\n",
    "\n",
    "    channels = kernel1.get_shape()[2]\n",
    "\n",
    "    # put NumKernels to the 1st dimension\n",
    "    x2 = tf.transpose(x1, (3, 0, 1, 2))\n",
    "    \n",
    "    # organize grid on Y axis\n",
    "    # x3 = tf.reshape(x2, tf.pack([grid_X, Y * grid_Y, X, channels])) #3\n",
    "    # In Tensorflow v1.0, changed to:\n",
    "    x3 = tf.reshape(x2, tf.stack([grid_X, Y * grid_Y, X, channels])) #3\n",
    "\n",
    "    # switch X and Y axes\n",
    "    x4 = tf.transpose(x3, (0, 2, 1, 3))\n",
    "    \n",
    "    # organize grid on X axis\n",
    "    # x5 = tf.reshape(x4, tf.pack([1, X * grid_X, Y * grid_Y, channels])) #3\n",
    "    # In Tensorflow v1.0, changed to:\n",
    "    x5 = tf.reshape(x4, tf.stack([1, X * grid_X, Y * grid_Y, channels])) #3\n",
    "    \n",
    "    # back to normal order (not combining with the next step for clarity)\n",
    "    x6 = tf.transpose(x5, (2, 1, 3, 0))\n",
    "\n",
    "    # to tf.image_summary order [batch_size, height, width, channels],\n",
    "    #   where in this case batch_size == 1\n",
    "    x7 = tf.transpose(x6, (3, 0, 1, 2))\n",
    "\n",
    "    # scale to [0, 255] and convert to uint8\n",
    "    return tf.image.convert_image_dtype(x7, dtype = tf.uint8) \n",
    "        \n",
    "# Helper function to create a Variable stored on CPU memory for convolution layer\n",
    "def _variable_on_cpu(name, shape, initializer):\n",
    "    \"\"\"Helper function to create a Variable stored on CPU memory.\n",
    "    Args:\n",
    "      name: name of the variable\n",
    "      shape: list of ints\n",
    "      initializer: initializer for Variable\n",
    "    Returns:\n",
    "      Variable Tensor\n",
    "    \"\"\"\n",
    "    with tf.device('/cpu:0'):\n",
    "        var = tf.get_variable(name, shape, initializer=initializer)\n",
    "    return var\n",
    "\n",
    "# Helper function to create a Variable stored on GPU memory for convolution layer\n",
    "def _variable_on_gpu(name, shape, initializer):\n",
    "    \"\"\"Helper to create a Variable stored on GPU memory.\n",
    "    Args:\n",
    "      name: name of the variable\n",
    "      shape: list of ints\n",
    "      initializer: initializer for Variable\n",
    "    Returns:\n",
    "      Variable Tensor\n",
    "    \"\"\"\n",
    "    var = tf.get_variable(name, shape, initializer=initializer)\n",
    "    return var\n",
    "\n",
    "# Helper function to create an initialized Variable with weight decay for convolution layer\n",
    "def _variable_with_weight_decay(name, shape, stddev, wd):\n",
    "    \"\"\"Helper to create an initialized Variable with weight decay.\n",
    "    Note that the Variable is initialized with a truncated normal distribution.\n",
    "    A weight decay is added only if one is specified.\n",
    "    Args:\n",
    "      name: name of the variable\n",
    "      shape: list of ints\n",
    "      stddev: standard deviation of a truncated Gaussian\n",
    "      wd: add L2Loss weight decay multiplied by this float. If None, weight\n",
    "          decay is not added for this Variable.\n",
    "    Returns:\n",
    "      Variable Tensor\n",
    "    \"\"\"\n",
    "    var = _variable_on_gpu(name, shape,\n",
    "                           tf.truncated_normal_initializer(stddev=stddev))\n",
    "    if wd:\n",
    "        weight_decay = tf.mul(tf.nn.l2_loss(var), wd, name='weight_loss')\n",
    "        tf.add_to_collection('losses', weight_decay)\n",
    "    return var\n",
    "\n",
    "# Helper function to create summaries for activations\n",
    "def _activation_summary(x):\n",
    "    \"\"\"Helper to create summaries for activations.\n",
    "    Creates a summary that provides a histogram of activations.\n",
    "    Creates a summary that measure the sparsity of activations.\n",
    "    Args:\n",
    "      x: Tensor\n",
    "    Returns:\n",
    "      nothing\n",
    "    \"\"\"\n",
    "    tensor_name = x.op.name\n",
    "    tf.summary.histogram(tensor_name + '/activations', x)\n",
    "    tf.summary.scalar(tensor_name + '/sparsity', tf.nn.zero_fraction(x))\n",
    "\n",
    "# Remade CNN model\n",
    "def cnn_remake(x_):\n",
    "    # Conv layer 1\n",
    "    with tf.variable_scope('conv1') as scope:\n",
    "        kernel = _variable_with_weight_decay(name='weights', shape = [5, 5, 3, 32], stddev=0.1, wd=0.0) # [width, height, dim input, filter output]\n",
    "        conv = tf.nn.conv2d(input=x_, filter=kernel, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
    "        biases = _variable_on_gpu(name='biases', shape=[32], initializer=tf.constant_initializer(0.0))\n",
    "        bias = tf.nn.bias_add(conv, biases)\n",
    "        conv1 = tf.nn.relu(bias, name=scope.name)\n",
    "        \n",
    "        # Tensorboard visualization\n",
    "        _activation_summary(conv1)\n",
    "        grid = put_kernels_on_grid (kernel, grid_X=4, grid_Y=8)\n",
    "        tf.summary.image(scope.name + 'kernel', grid, max_outputs=1)\n",
    "        \n",
    "        \n",
    "    # Pooling layer 1\n",
    "    with tf.variable_scope('pool1') as scope:\n",
    "        pool1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "    \n",
    "    # Conv layer 2\n",
    "    with tf.variable_scope('conv2') as scope:\n",
    "        kernel = _variable_with_weight_decay(name='weights', shape = [5, 5, 32, 32], stddev=0.1, wd=0.0) # [width, height, dim input, filter output]\n",
    "        \n",
    "        conv = tf.nn.conv2d(input=pool1, filter=kernel, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
    "        biases = _variable_on_gpu(name='biases', shape=[32], initializer=tf.constant_initializer(0.1))\n",
    "        bias = tf.nn.bias_add(conv, biases)\n",
    "        conv2 = tf.nn.relu(bias, name=scope.name)\n",
    "        \n",
    "        # Tensorboard visualization\n",
    "        _activation_summary(conv2)\n",
    "        # grid = put_kernels_on_grid (kernel, grid_X=4, grid_Y=8)\n",
    "        # tf.summary.image(scope.name + 'kernel', grid, max_outputs=1)\n",
    "        \n",
    "    # Pooling layer 2\n",
    "    with tf.variable_scope('pool2') as scope:\n",
    "        pool2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "    \n",
    "    # Conv layer 3\n",
    "    with tf.variable_scope('conv3') as scope:\n",
    "        kernel = _variable_with_weight_decay(name='weights', shape = [5, 5, 32, 32], stddev=0.1, wd=0.0) # [width, height, dim input, filter output]\n",
    "        \n",
    "        conv = tf.nn.conv2d(input=pool2, filter=kernel, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
    "        biases = _variable_on_gpu(name='biases', shape=[32], initializer=tf.constant_initializer(0.1))\n",
    "        bias = tf.nn.bias_add(conv, biases)\n",
    "        conv3 = tf.nn.relu(bias, name=scope.name)\n",
    "        \n",
    "        # Tensorboard visualization\n",
    "        _activation_summary(conv3)\n",
    "        # grid = put_kernels_on_grid (kernel, grid_X=4, grid_Y=8)\n",
    "        # tf.summary.image(scope.name + 'kernel', grid, max_outputs=1)\n",
    "                \n",
    "    # Pooling layer 3\n",
    "    with tf.variable_scope('pool3') as scope:\n",
    "        pool3 = tf.nn.max_pool(conv3, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "    \n",
    "    # Flatten layer: Flattens the input while maintaining the batch_size.\n",
    "    with tf.variable_scope('pool-flat') as scope:\n",
    "        pool_flat = tf.contrib.layers.flatten(pool3, scope='pool3flat')\n",
    "    \n",
    "    # Fully-connected layer 1\n",
    "    with tf.variable_scope('fc1') as scope:\n",
    "        fc1 = tf.layers.dense(inputs=pool_flat, units=500, activation=tf.nn.relu)\n",
    "    \n",
    "    # Fully-connected layer 2: output logits\n",
    "    with tf.variable_scope('logits') as scope:\n",
    "        logits = tf.layers.dense(inputs=fc1, units=10)\n",
    "        \n",
    "    return logits\n",
    "\n",
    "def apply_classification_loss(model_function):\n",
    "    with tf.Graph().as_default() as g:\n",
    "#         with tf.device(\"/gpu:0\"):  # use gpu:0 if on GPU\n",
    "            x_ = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "            y_ = tf.placeholder(tf.int32, [None])\n",
    "            y_logits = model_function(x_)\n",
    "            \n",
    "            y_dict = dict(labels=y_, logits=y_logits)\n",
    "            losses = tf.nn.sparse_softmax_cross_entropy_with_logits(**y_dict)\n",
    "            cross_entropy_loss = tf.reduce_mean(losses)\n",
    "            \n",
    "            # TensorBoard operation\n",
    "            tf.summary.scalar('cross_entropy_loss', cross_entropy_loss)\n",
    "            \n",
    "            trainer = tf.train.AdamOptimizer()\n",
    "            train_op = trainer.minimize(cross_entropy_loss)\n",
    "            \n",
    "            y_pred = tf.argmax(tf.nn.softmax(y_logits), dimension=1)\n",
    "            correct_prediction = tf.equal(tf.cast(y_pred, tf.int32), y_)\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "            \n",
    "            # TensorBoard operation\n",
    "            tf.summary.scalar('accuracy', accuracy)\n",
    "            \n",
    "            # Tensorboard merge all ops\n",
    "            merged = tf.summary.merge_all()\n",
    "            \n",
    "    model_dict = {'graph': g, 'inputs': [x_, y_], 'train_op': train_op,\n",
    "                  'accuracy': accuracy, 'loss': cross_entropy_loss, 'merged_summary':merged}\n",
    "    \n",
    "    return model_dict\n",
    "\n",
    "\n",
    "def train_model(model_dict, dataset_generators, epoch_n, print_every):\n",
    "    with model_dict['graph'].as_default(), tf.Session() as sess:\n",
    "        \n",
    "        # TensorBoard\n",
    "        train_writer = tf.summary.FileWriter('./logs/train', sess.graph)\n",
    "        \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        for epoch_i in range(epoch_n):\n",
    "            for iter_i, data_batch in enumerate(dataset_generators['train']):\n",
    "                train_feed_dict = dict(zip(model_dict['inputs'], data_batch))\n",
    "                \n",
    "                # Tensorboard Train\n",
    "                train_summary, _ = sess.run([model_dict['merged_summary'], model_dict['train_op']], feed_dict=train_feed_dict)\n",
    "                \n",
    "                # Tensorboard train writer\n",
    "                train_writer.add_summary(train_summary, iter_i)\n",
    "                \n",
    "                if iter_i % print_every == 0:\n",
    "                    collect_arr = []\n",
    "                    for test_batch in dataset_generators['test']:\n",
    "                        test_feed_dict = dict(zip(model_dict['inputs'], test_batch))\n",
    "                        to_compute = [model_dict['loss'], model_dict['accuracy']]\n",
    "                        collect_arr.append(sess.run(to_compute, test_feed_dict))\n",
    "                    averages = np.mean(collect_arr, axis=0)\n",
    "                    fmt = (epoch_i, iter_i, ) + tuple(averages)\n",
    "                    print('epoch {:d} iter {:d}, loss: {:.3f}, '\n",
    "                          'accuracy: {:.3f}'.format(*fmt))\n",
    "def run():\n",
    "    dataset_generators = {\n",
    "        'train': svhn_dataset_generator('train', 256),\n",
    "        'test': svhn_dataset_generator('test', 256)\n",
    "    }\n",
    "    \n",
    "    model_dict = apply_classification_loss(cnn_remake)\n",
    "    train_model(model_dict, dataset_generators, epoch_n=50, print_every=200)\n",
    "\n",
    "run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some visualization results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now if we just use `tensorboard --logdir=./logs/` (on linux based system), we could get following results:\n",
    "\n",
    "**Accuracy**\n",
    "\n",
    "<img src=\"accur.png\" style=\"height:50%; width:50%;\">\n",
    "\n",
    "**Loss**\n",
    "\n",
    "<img src=\"loss.png\" sytle=\"width:400px\";>\n",
    "\n",
    "**Filter (Convolutional layer1 as example)**\n",
    "\n",
    "<img src=\"filter1.png\" sytle=\"width:400px\";>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Bonus\n",
    "(20 points)\n",
    "\n",
    "### Q5.1 SVHN Net ++\n",
    "Improve the accuracy of SVHN Net beyond that of the provided demo: SVHN Net ++."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cnn_bonus(x_):\n",
    "    \n",
    "    # 3 layers seems better 2 layers; not tried 4 layers yet\n",
    "    # stride 1 seems better than 2 and larger\n",
    "    # filter = 16 seems better than 32 and larger\n",
    "    \n",
    "    #\n",
    "    # The first convolution layer produces 16 features with 5x5 convolution filters\n",
    "    # The second convolution layer outputs 512 features with 7x7 filters\n",
    "    # The classifier is a 2-layer non-linear classifier with 20 hidden units\n",
    "    \n",
    "    # We use stochastic gradient descent as our optimization method and shuffle our dataset after\n",
    "    # each training iteration\n",
    "    \n",
    "    # pooling layers: p = 2, 4, 12 give the best performance\n",
    "    \n",
    "    with tf.variable_scope('conv1'):\n",
    "        conv1 = tf.layers.conv2d(\n",
    "                inputs=x_,\n",
    "                filters=32,  # number of filters\n",
    "                kernel_size=[7, 7],\n",
    "                padding=\"same\",\n",
    "                activation=tf.nn.relu)\n",
    "\n",
    "        pool1 = tf.layers.max_pooling2d(inputs=conv1, \n",
    "                                        pool_size=[2, 2], \n",
    "                                        strides=2)  # convolution stride\n",
    "    \n",
    "    conv2 = tf.layers.conv2d(\n",
    "            name='conv2',\n",
    "            inputs=pool1,\n",
    "            filters=32, # number of filters\n",
    "            kernel_size=[5, 5],\n",
    "            padding=\"same\",\n",
    "            activation=tf.nn.relu)\n",
    "    \n",
    "    pool2 = tf.layers.max_pooling2d(name='pool2',\n",
    "                                    inputs=conv2, \n",
    "                                    pool_size=[2, 2], \n",
    "                                    strides=2)  # convolution stride\n",
    "    \n",
    "    conv3 = tf.layers.conv2d(\n",
    "            name='conv3',\n",
    "            inputs=pool2,\n",
    "            filters=64, # number of filters\n",
    "            kernel_size=[5, 5],\n",
    "            padding=\"same\",\n",
    "            activation=tf.nn.relu)\n",
    "    \n",
    "    pool3 = tf.layers.max_pooling2d(name='pool3',\n",
    "                                    inputs=conv3, \n",
    "                                    pool_size=[2, 2], \n",
    "                                    strides=2)  # convolution stride\n",
    "    \n",
    "    conv4 = tf.layers.conv2d(\n",
    "            name='conv4',\n",
    "            inputs=pool3,\n",
    "            filters=64, # number of filters\n",
    "            kernel_size=[5, 5],\n",
    "            padding=\"same\",\n",
    "            activation=tf.nn.relu)\n",
    "    \n",
    "    pool4 = tf.layers.max_pooling2d(name='pool4',\n",
    "                                    inputs=conv4, \n",
    "                                    pool_size=[2, 2], \n",
    "                                    strides=2)  # convolution stride\n",
    "        \n",
    "    pool_flat = tf.contrib.layers.flatten(pool4, scope='pool4flat')\n",
    "    dense = tf.layers.dense(inputs=pool_flat, units=500, activation=tf.nn.relu)\n",
    "    logits = tf.layers.dense(inputs=dense, units=10)\n",
    "    return logits\n",
    "\n",
    "\n",
    "def apply_classification_loss(model_function):\n",
    "    with tf.Graph().as_default() as g:\n",
    "        with tf.device(\"/gpu:0\"):  # use gpu:0 if on GPU\n",
    "            x_ = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "            y_ = tf.placeholder(tf.int32, [None])\n",
    "            y_logits = model_function(x_)\n",
    "            \n",
    "            y_dict = dict(labels=y_, logits=y_logits)\n",
    "            losses = tf.nn.sparse_softmax_cross_entropy_with_logits(**y_dict)\n",
    "            cross_entropy_loss = tf.reduce_mean(losses)\n",
    "            trainer = tf.train.AdamOptimizer()\n",
    "            train_op = trainer.minimize(cross_entropy_loss)\n",
    "            \n",
    "            y_pred = tf.argmax(tf.nn.softmax(y_logits), dimension=1)\n",
    "            correct_prediction = tf.equal(tf.cast(y_pred, tf.int32), y_)\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    model_dict = {'graph': g, 'inputs': [x_, y_], 'train_op': train_op,\n",
    "                  'accuracy': accuracy, 'loss': cross_entropy_loss}\n",
    "    \n",
    "    return model_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 iter 0, loss: 11.354, accuracy: 0.142\n",
      "epoch 0 iter 200, loss: 0.758, accuracy: 0.772\n",
      "epoch 1 iter 0, loss: 0.711, accuracy: 0.790\n",
      "epoch 1 iter 200, loss: 0.558, accuracy: 0.838\n",
      "epoch 2 iter 0, loss: 0.526, accuracy: 0.849\n",
      "epoch 2 iter 200, loss: 0.478, accuracy: 0.868\n",
      "epoch 3 iter 0, loss: 0.478, accuracy: 0.866\n",
      "epoch 3 iter 200, loss: 0.495, accuracy: 0.863\n",
      "epoch 4 iter 0, loss: 0.467, accuracy: 0.872\n",
      "epoch 4 iter 200, loss: 0.456, accuracy: 0.876\n",
      "epoch 5 iter 0, loss: 0.475, accuracy: 0.872\n",
      "epoch 5 iter 200, loss: 0.460, accuracy: 0.883\n",
      "epoch 6 iter 0, loss: 0.483, accuracy: 0.882\n",
      "epoch 6 iter 200, loss: 0.496, accuracy: 0.883\n",
      "epoch 7 iter 0, loss: 0.524, accuracy: 0.872\n",
      "epoch 7 iter 200, loss: 0.509, accuracy: 0.877\n",
      "epoch 8 iter 0, loss: 0.547, accuracy: 0.872\n",
      "epoch 8 iter 200, loss: 0.525, accuracy: 0.880\n",
      "epoch 9 iter 0, loss: 0.520, accuracy: 0.874\n",
      "epoch 9 iter 200, loss: 0.531, accuracy: 0.884\n",
      "epoch 10 iter 0, loss: 0.544, accuracy: 0.873\n",
      "epoch 10 iter 200, loss: 0.551, accuracy: 0.881\n",
      "epoch 11 iter 0, loss: 0.536, accuracy: 0.876\n",
      "epoch 11 iter 200, loss: 0.542, accuracy: 0.887\n",
      "epoch 12 iter 0, loss: 0.544, accuracy: 0.883\n",
      "epoch 12 iter 200, loss: 0.563, accuracy: 0.885\n",
      "epoch 13 iter 0, loss: 0.569, accuracy: 0.880\n",
      "epoch 13 iter 200, loss: 0.591, accuracy: 0.878\n",
      "epoch 14 iter 0, loss: 0.578, accuracy: 0.881\n",
      "epoch 14 iter 200, loss: 0.623, accuracy: 0.871\n",
      "epoch 15 iter 0, loss: 0.651, accuracy: 0.882\n",
      "epoch 15 iter 200, loss: 0.633, accuracy: 0.860\n",
      "epoch 16 iter 0, loss: 0.589, accuracy: 0.881\n",
      "epoch 16 iter 200, loss: 0.667, accuracy: 0.875\n",
      "epoch 17 iter 0, loss: 0.644, accuracy: 0.877\n",
      "epoch 17 iter 200, loss: 0.689, accuracy: 0.874\n",
      "epoch 18 iter 0, loss: 0.666, accuracy: 0.870\n",
      "epoch 18 iter 200, loss: 0.736, accuracy: 0.870\n",
      "epoch 19 iter 0, loss: 0.661, accuracy: 0.876\n",
      "epoch 19 iter 200, loss: 0.697, accuracy: 0.880\n",
      "epoch 20 iter 0, loss: 0.720, accuracy: 0.885\n",
      "epoch 20 iter 200, loss: 0.747, accuracy: 0.883\n",
      "epoch 21 iter 0, loss: 0.729, accuracy: 0.882\n",
      "epoch 21 iter 200, loss: 0.742, accuracy: 0.879\n",
      "epoch 22 iter 0, loss: 0.797, accuracy: 0.876\n",
      "epoch 22 iter 200, loss: 0.819, accuracy: 0.875\n",
      "epoch 23 iter 0, loss: 0.711, accuracy: 0.882\n",
      "epoch 23 iter 200, loss: 0.763, accuracy: 0.880\n",
      "epoch 24 iter 0, loss: 0.759, accuracy: 0.884\n",
      "epoch 24 iter 200, loss: 0.792, accuracy: 0.886\n",
      "epoch 25 iter 0, loss: 0.863, accuracy: 0.882\n",
      "epoch 25 iter 200, loss: 0.811, accuracy: 0.878\n",
      "epoch 26 iter 0, loss: 0.798, accuracy: 0.879\n",
      "epoch 26 iter 200, loss: 0.860, accuracy: 0.885\n",
      "epoch 27 iter 0, loss: 0.794, accuracy: 0.874\n",
      "epoch 27 iter 200, loss: 0.830, accuracy: 0.888\n",
      "epoch 28 iter 0, loss: 0.884, accuracy: 0.884\n",
      "epoch 28 iter 200, loss: 0.964, accuracy: 0.882\n",
      "epoch 29 iter 0, loss: 0.873, accuracy: 0.890\n",
      "epoch 29 iter 200, loss: 0.953, accuracy: 0.875\n",
      "epoch 30 iter 0, loss: 0.968, accuracy: 0.869\n",
      "epoch 30 iter 200, loss: 0.984, accuracy: 0.876\n",
      "epoch 31 iter 0, loss: 0.828, accuracy: 0.886\n",
      "epoch 31 iter 200, loss: 1.067, accuracy: 0.881\n",
      "epoch 32 iter 0, loss: 0.925, accuracy: 0.880\n",
      "epoch 32 iter 200, loss: 0.974, accuracy: 0.881\n",
      "epoch 33 iter 0, loss: 0.856, accuracy: 0.882\n",
      "epoch 33 iter 200, loss: 0.997, accuracy: 0.885\n",
      "epoch 34 iter 0, loss: 0.997, accuracy: 0.886\n",
      "epoch 34 iter 200, loss: 1.063, accuracy: 0.881\n",
      "epoch 35 iter 0, loss: 1.071, accuracy: 0.885\n",
      "epoch 35 iter 200, loss: 0.940, accuracy: 0.886\n",
      "epoch 36 iter 0, loss: 0.997, accuracy: 0.880\n",
      "epoch 36 iter 200, loss: 1.021, accuracy: 0.884\n",
      "epoch 37 iter 0, loss: 1.091, accuracy: 0.886\n",
      "epoch 37 iter 200, loss: 1.079, accuracy: 0.886\n",
      "epoch 38 iter 0, loss: 1.131, accuracy: 0.884\n",
      "epoch 38 iter 200, loss: 0.946, accuracy: 0.892\n",
      "epoch 39 iter 0, loss: 1.184, accuracy: 0.876\n",
      "epoch 39 iter 200, loss: 1.094, accuracy: 0.889\n",
      "epoch 40 iter 0, loss: 1.060, accuracy: 0.887\n",
      "epoch 40 iter 200, loss: 1.054, accuracy: 0.888\n",
      "epoch 41 iter 0, loss: 1.212, accuracy: 0.875\n",
      "epoch 41 iter 200, loss: 1.065, accuracy: 0.884\n",
      "epoch 42 iter 0, loss: 1.174, accuracy: 0.874\n",
      "epoch 42 iter 200, loss: 1.109, accuracy: 0.882\n",
      "epoch 43 iter 0, loss: 1.105, accuracy: 0.881\n",
      "epoch 43 iter 200, loss: 1.022, accuracy: 0.889\n",
      "epoch 44 iter 0, loss: 1.101, accuracy: 0.879\n",
      "epoch 44 iter 200, loss: 1.036, accuracy: 0.887\n",
      "epoch 45 iter 0, loss: 1.102, accuracy: 0.892\n",
      "epoch 45 iter 200, loss: 1.095, accuracy: 0.884\n",
      "epoch 46 iter 0, loss: 1.111, accuracy: 0.891\n",
      "epoch 46 iter 200, loss: 1.046, accuracy: 0.891\n",
      "epoch 47 iter 0, loss: 1.146, accuracy: 0.890\n",
      "epoch 47 iter 200, loss: 1.150, accuracy: 0.886\n",
      "epoch 48 iter 0, loss: 1.140, accuracy: 0.881\n",
      "epoch 48 iter 200, loss: 1.146, accuracy: 0.886\n",
      "epoch 49 iter 0, loss: 1.238, accuracy: 0.883\n",
      "epoch 49 iter 200, loss: 0.996, accuracy: 0.886\n"
     ]
    }
   ],
   "source": [
    "def SVHN_plusplus():\n",
    "    model_dict = apply_classification_loss(cnn_bonus)\n",
    "    train_model(model_dict, dataset_generators, epoch_n=50, print_every=200)\n",
    "SVHN_plusplus()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
